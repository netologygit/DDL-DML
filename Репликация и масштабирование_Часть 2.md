Репликация и масштабирование. Часть 2 - Игуменова Екатерина

***Задание 1***


***1 Мастер - реплика***: при данном варианте мы можем снизить нагрузку на мастера, предоставив доступ таким образом, чтобы пользоваели могли выполнять чтение только с реплики. Мастер будет доступен для insert, update, delete, а реплика для select. Также мы сможем в случае падения мастера быстро переключиться на реплику ( в случае синхронной реплики, мы полностью сохраним наши данные, в случае асинхронной - потеряем незначительное кол-во данных). Синхронная или асинхронная реплика зависит от критичности данных для проекта. критично ли потерять данный за небольшой промежуток времени или нет)


***2 Мастер и несколько реплик***: идеальный вариант также для снижения нагрузки на мастера, плюс синхронная  реплика всегда готова стать мастером бнз потери данных, а асинхронная реплика так и останется доступна для чтения, тем самым разгружая нашего нового мастера. пока происходит восстановление 3 ноды.


***3.DRBD*** - это распределенная система хранения на уровне хранения ядра Linux. DRBD может использоваться для совместного использования блочных устройств, файловых систем и данных между двумя серверами Linux. Аналогично функции сетевого RAID-1. Т е данный вариант дает дополнительное преимущество  в увеличении отказоустойчивости.

***4. К основным преимуществам SAN*** можно отнести практически все ее особенности:
Независимость топологии SAN от сторедж-систем и серверов

Удобное централизованное управление

Отсутствие конфликта с трафиком LAN/WAN

Удобное резервирование данных без загрузки локальной сети и серверов

Высокое быстродействие

Высокая масштабируемость

Высокая гибкость

Высокая готовность и отказоустойчивость


***Задание 2***


При вертикальном шардинге, все наши таблицы находятся на одном инстансе.
Мы можем создать три БД: книги, пользователи, магазины.
К примеру, у нас огромная таблица с пользователями.
Нам необходимо использовать партицирование, т е мы одну большую таблицу разделим на много маленьких по какому-либо принципу
К примеру, таблицу с юзерами мы разделим ,например, по user_id. У нас будет две табицы, user и user_1
Нам нужно будет сделать 2 действия над табличкой — это поставить у нашего шарда, например, user_1 то, 
что она будет наследоваться таблицей user. User будет базовой таблицей, будет содержать всю структуру, и мы, 
создавая партицию, будем указывать, что она наследуется нашей базовой таблицей. Наследованная таблица будет 
иметь все колонки родителя — той базовой таблицы, которую мы указали, а также она может иметь свои колонки, 
которые мы дополнительно туда добавим. Она будет полноценной таблицей, но унаследованной от родителя, 
и там не будет ограничений, индексов и триггеров от родителя/
2-ое действие, которое нужно сделать — это поставить ограничения. Это будет проверка, что в эту таблицу будут попадать 
данные только по нашему выбранному признаку: user_id. Партиции в таблицах нужно обязательно создавать по одинаковым полям
и четко прописывать ограничения. 

![Screenshot_1](https://github.com/netologygit/DDL-DML/assets/123411071/eefd1aac-e0bb-43ae-9566-b5a5fb9214d5)


Горизонтальное шардирование, когда мы наши данные разносим на разные сервера. В идеале каждый кластер у нас состоит, как миниумум из 3 нод - мастер, синхронной и асинхронной реплики,
в случае падения мастера у нас синхронная реплика быстро возьмет на себя роль мастера, наши данные останутся консиcтентны.
А вот между двумя кластерами, на которых у нас разные шарды, связи нет никакой, они совершенно независимы друг от друга.
При наличии у нас двух кластеров с разными шардингами, бэкап на кластере 1 и кластере2 запускаются по-разному, даже если нам кажется, что это происходит одномоментно. 
В то время, когда у нас происходит бэкап на кластере 1, на кластере 2 могут обновиться какие-то данные, и если у нас таблица с 1 шардинга взаимосвязана с данными на
шардинге 2, то может получиться так, что на шарде 1 у нас есть данные, а на шарде 2 их еще нет, тем самым целостность данных нарушится.
В итоге основные проблемы при использовании горизонтального шардинга: как бэкапить кластеры и как потом восстанавливать, чтобы 
не нарушилась целостность данных и данные были консистентны.
Горизонтальное шардирование на практике можно использовать, когда наши данные разбиты, к примеру, по городам. 
Т е 1-ый шардинг - Москва, 2 шардинг - Питер, 3 шардинг - Казань.
Наши данные совершенно не будут пересекаться, при этом мы сможем снизить нагрузку на сервера, распределив запросы пользователей на разные кластера в соответствии с
геолокацией. А уже внутри каждого кластера мы можем создавать партицирование наших таблиц, используя вертикальное шардирование (как на рисунке 1). Наши данные в каждом кластре будут консистентны, мы легко сможем восстановить данные из бэкапа на каждом кластере, при этом не соприкасаясь с другими кластерами.


![Screenshot_2](https://github.com/netologygit/DDL-DML/assets/123411071/b2783553-399f-4ba8-9a8a-1e963c7b42dd)



